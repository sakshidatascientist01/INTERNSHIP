{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3a1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product):\n",
    "    # Format the user input as a URL-friendly string\n",
    "    formatted_product = '+'.join(product.split())\n",
    "\n",
    "    # Amazon India URL for product search\n",
    "    url = f\"https://www.amazon.in/s?k={formatted_product}\"\n",
    "\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = []\n",
    "\n",
    "        # Find all product elements on the search results page\n",
    "        product_elements = soup.find_all('div', class_='s-result-item')\n",
    "\n",
    "        for product_element in product_elements:\n",
    "            # Extract product details like title and price\n",
    "            title_element = product_element.find('span', class_='a-text-normal')\n",
    "            price_element = product_element.find('span', class_='a-offscreen')\n",
    "\n",
    "            if title_element and price_element:\n",
    "                product_title = title_element.text.strip()\n",
    "                product_price = price_element.text.strip()\n",
    "\n",
    "                # Append product details to the list\n",
    "                products.append({'Title': product_title, 'Price': product_price})\n",
    "\n",
    "        return products\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from Amazon.in\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    product_to_search = input(\"Enter the product you want to search for on Amazon.in: \")\n",
    "    search_results = search_amazon(product_to_search)\n",
    "\n",
    "    if search_results:\n",
    "        print(f\"Search results for '{product_to_search}':\")\n",
    "        for i, product in enumerate(search_results, start=1):\n",
    "            print(f\"{i}. {product['Title']} - Price: {product['Price']}\")\n",
    "    else:\n",
    "        print(\"No search results found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289a5af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer2 \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_product_details(product):\n",
    "    formatted_product = '+'.join(product.split())\n",
    "    products = []\n",
    "\n",
    "    for page_number in range(1, 4):  # Scrape the first 3 pages\n",
    "        url = f\"https://www.amazon.in/s?k={formatted_product}&page={page_number}\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            product_elements = soup.find_all('div', class_='s-result-item')\n",
    "\n",
    "            for product_element in product_elements:\n",
    "                details = {}\n",
    "\n",
    "                # Extract product details\n",
    "                title_element = product_element.find('span', class_='a-text-normal')\n",
    "                price_element = product_element.find('span', class_='a-price')\n",
    "                return_exchange_element = product_element.find('div', class_='s-return-policy-info')\n",
    "                expected_delivery_element = product_element.find('span', class_='a-text-bold')\n",
    "                availability_element = product_element.find('span', class_='a-size-base')\n",
    "\n",
    "                if title_element:\n",
    "                    details['Name of the Product'] = title_element.text.strip()\n",
    "                if price_element:\n",
    "                    details['Price'] = price_element.find('span', class_='a-offscreen').text.strip()\n",
    "                if return_exchange_element:\n",
    "                    details['Return/Exchange'] = return_exchange_element.text.strip()\n",
    "                if expected_delivery_element:\n",
    "                    details['Expected Delivery'] = expected_delivery_element.text.strip()\n",
    "                if availability_element:\n",
    "                    details['Availability'] = availability_element.text.strip()\n",
    "\n",
    "                # Extract product URL\n",
    "                product_link = product_element.find('a', class_='a-link-normal', href=True)\n",
    "                if product_link:\n",
    "                    details['Product URL'] = 'https://www.amazon.in' + product_link['href']\n",
    "\n",
    "                # Append product details to the list\n",
    "                products.append(details)\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data from Amazon.in for page {page_number}\")\n",
    "\n",
    "    return products\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    product_to_search = input(\"Enter the product you want to search for on Amazon.in: \")\n",
    "    search_results = scrape_product_details(product_to_search)\n",
    "\n",
    "    if search_results:\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(search_results)\n",
    "\n",
    "        # Replace missing values with \"-\"\n",
    "        df.fillna('-', inplace=True)\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(f'{product_to_search}_search_results.csv', index=False)\n",
    "        print(f\"Search results for '{product_to_search}' have been saved to '{product_to_search}_search_results.csv'.\")\n",
    "    else:\n",
    "        print(\"No search results found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8b734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer3\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set the path to the Chrome WebDriver executable\n",
    "chromedriver_path = https://www.google.com/chrome/\n",
    "\n",
    "# Create a directory to save the downloaded images\n",
    "output_directory = 'downloaded_images'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Keywords to search for\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome(chromedriver_path)\n",
    "\n",
    "# Loop through each keyword and scrape 10 images\n",
    "for keyword in keywords:\n",
    "    # Navigate to Google Images\n",
    "    driver.get(\"https://www.google.com/imghp\")\n",
    "\n",
    "    # Find the search bar element and enter the keyword\n",
    "    search_bar = driver.find_element_by_name(\"q\")\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(keyword)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Scroll down to load more images (adjust the number of scrolls as needed)\n",
    "    num_scrolls = 3\n",
    "    for _ in range(num_scrolls):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "    # Find image elements on the page\n",
    "    image_elements = driver.find_elements_by_css_selector(\".rg_i\")\n",
    "\n",
    "    # Download the first 10 images\n",
    "    num_images_to_download = 10\n",
    "    for i, image_element in enumerate(image_elements[:num_images_to_download]):\n",
    "        image_url = image_element.get_attribute(\"src\")\n",
    "        if image_url and image_url.startswith(\"http\"):\n",
    "            # Download the image and save it in the specified directory\n",
    "            image_path = os.path.join(output_directory, f\"{keyword}_{i + 1}.jpg\")\n",
    "            with open(image_path, \"wb\") as img_file:\n",
    "                img_file.write(requests.get(image_url).content)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c9f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#amswer 4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_smartphones(search_query):\n",
    "    url = f\"https://www.flipkart.com/search?q={search_query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Your User Agent Here\"  # Replace with your user agent\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        products = []\n",
    "\n",
    "        product_elements = soup.find_all('div', class_='_1AtVbE')\n",
    "        \n",
    "        for product_element in product_elements:\n",
    "            details = {}\n",
    "            \n",
    "            # Extract product details\n",
    "            details['Brand Name'] = product_element.find('div', class_='_4rR01T').text.strip()\n",
    "            details['Smartphone name'] = product_element.find('a', class_='IRpwTa').text.strip()\n",
    "            details['Colour'] = product_element.find('a', class_='_2W-UZw').text.strip()\n",
    "            details['Price'] = product_element.find('div', class_='_30jeq3').text.strip()\n",
    "            \n",
    "            # Extract product URL\n",
    "            product_url = product_element.find('a', class_='IRpwTa')['href']\n",
    "            details['Product URL'] = f\"https://www.flipkart.com{product_url}\"\n",
    "            \n",
    "            # Go to the individual product page to extract more details\n",
    "            product_response = requests.get(details['Product URL'], headers=headers)\n",
    "            if product_response.status_code == 200:\n",
    "                product_soup = BeautifulSoup(product_response.content, 'html.parser')\n",
    "                specs_elements = product_soup.find_all('li', class_='rgWa7D')\n",
    "                for spec_element in specs_elements:\n",
    "                    spec_text = spec_element.text.strip()\n",
    "                    if 'RAM' in spec_text:\n",
    "                        details['RAM'] = spec_text\n",
    "                    elif 'ROM' in spec_text:\n",
    "                        details['Storage(ROM)'] = spec_text\n",
    "                    elif 'Primary Camera' in spec_text:\n",
    "                        details['Primary Camera'] = spec_text\n",
    "                    elif 'Secondary Camera' in spec_text:\n",
    "                        details['Secondary Camera'] = spec_text\n",
    "                    elif 'Display Size' in spec_text:\n",
    "                        details['Display Size'] = spec_text\n",
    "                    elif 'Battery Capacity' in spec_text:\n",
    "                        details['Battery Capacity'] = spec_text\n",
    "            \n",
    "            # Append product details to the list\n",
    "            products.append(details)\n",
    "            \n",
    "        return products\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from Flipkart\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the smartphone you want to search for on Flipkart: \")\n",
    "    search_results = scrape_flipkart_smartphones(search_query)\n",
    "\n",
    "    if search_results:\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(search_results)\n",
    "\n",
    "        # Replace missing values with \"-\"\n",
    "        df.fillna('-', inplace=True)\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(f'{search_query}_search_results.csv', index=False)\n",
    "        print(f\"Search results for '{search_query}' have been saved to '{search_query}_search_results.csv'.\")\n",
    "    else:\n",
    "        print(\"No search results found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer 5\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Set the path to the Chrome WebDriver executable\n",
    "chromedriver_path = https://www.google.com/chrome/\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome(chromedriver_path)\n",
    "\n",
    "def get_coordinates(city_name):\n",
    "    try:\n",
    "        # Open Google Maps\n",
    "        driver.get(\"https://maps.google.com\")\n",
    "\n",
    "        # Find the search bar element and enter the city name\n",
    "        search_bar = driver.find_element_by_name(\"q\")\n",
    "        search_bar.clear()\n",
    "        search_bar.send_keys(city_name)\n",
    "        search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "        # Wait for the map to load\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        # Find the \"Share\" button and click it\n",
    "        share_button = driver.find_element_by_css_selector(\".widget-scene-footer button.widget-action-icon-button\")\n",
    "        share_button.click()\n",
    "\n",
    "        # Find and extract the URL containing the coordinates\n",
    "        url = driver.current_url\n",
    "        coordinates = url.split(\"@\")[1].split(\",\")[0:2]\n",
    "\n",
    "        # Extract latitude and longitude\n",
    "        latitude = coordinates[0]\n",
    "        longitude = coordinates[1]\n",
    "\n",
    "        return latitude, longitude\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city_name = input(\"Enter the name of the city: \")\n",
    "    latitude, longitude = get_coordinates(city_name)\n",
    "\n",
    "    if latitude is not None and longitude is not None:\n",
    "        print(f\"Coordinates for {city_name}: Latitude {latitude}, Longitude {longitude}\")\n",
    "    else:\n",
    "        print(f\"Coordinates for {city_name} could not be found.\")\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer 6 \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the page containing the best gaming laptops\n",
    "url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    laptops = []\n",
    "\n",
    "    # Find all laptop details on the page\n",
    "    laptop_elements = soup.find_all('div', class_='TopNumbeHeading active-class ')\n",
    "    \n",
    "    for laptop_element in laptop_elements:\n",
    "        details = {}\n",
    "        \n",
    "        # Extract laptop details\n",
    "        details['Product Name'] = laptop_element.find('div', class_='Top-box').h3.a.text.strip()\n",
    "        details['Price'] = laptop_element.find('div', class_='swPr').text.strip()\n",
    "        details['Processor'] = laptop_element.find('div', class_='prdouct-detail').ul.li.text.strip()\n",
    "        details['RAM'] = laptop_element.find('div', class_='prdouct-detail').ul.li.find_next('li').text.strip()\n",
    "        details['OS'] = laptop_element.find('div', class_='prdouct-detail').ul.li.find_next('li').find_next('li').text.strip()\n",
    "        details['Storage'] = laptop_element.find('div', class_='prdouct-detail').ul.li.find_next('li').find_next('li').find_next('li').text.strip()\n",
    "        details['Display'] = laptop_element.find('div', class_='prdouct-detail').ul.li.find_next('li').find_next('li').find_next('li').find_next('li').text.strip()\n",
    "        details['Graphics'] = laptop_element.find('div', class_='prdouct-detail').ul.li.find_next('li').find_next('li').find_next('li').find_next('li').find_next('li').text.strip()\n",
    "\n",
    "        # Append laptop details to the list\n",
    "        laptops.append(details)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(laptops)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('best_gaming_laptops.csv', index=False)\n",
    "    print(\"Details of the best gaming laptops have been saved to 'best_gaming_laptops.csv'.\")\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve data from digit.in\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa755ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer7\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the Forbes billionaires list\n",
    "url = \"https://www.forbes.com/billionaires/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    billionaires = []\n",
    "\n",
    "    # Find all billionaire details on the page\n",
    "    billionaire_elements = soup.find_all('div', class_='personList')\n",
    "\n",
    "    for element in billionaire_elements:\n",
    "        details = {}\n",
    "        \n",
    "        # Extract billionaire details\n",
    "        details['Rank'] = element.find('div', class_='rank').text.strip()\n",
    "        details['Name'] = element.find('div', class_='personName').text.strip()\n",
    "        details['Net worth'] = element.find('div', class_='netWorth').text.strip()\n",
    "        details['Age'] = element.find('div', class_='age').text.strip()\n",
    "        details['Citizenship'] = element.find('div', class_='countryOfCitizenship').text.strip()\n",
    "        details['Source'] = element.find('div', class_='source').text.strip()\n",
    "        details['Industry'] = element.find('div', class_='category').text.strip()\n",
    "\n",
    "        # Append billionaire details to the list\n",
    "        billionaires.append(details)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(billionaires)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('forbes_billionaires.csv', index=False)\n",
    "    print(\"Details of billionaires have been saved to 'forbes_billionaires.csv'.\")\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve data from Forbes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be43c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer 8 (wasnt able to dop this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer 9 \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the hostel listings in London\n",
    "url = \"https://www.hostelworld.com/s?q=London&country=England\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    hostels = []\n",
    "\n",
    "    # Find all hostel elements on the page\n",
    "    hostel_elements = soup.find_all('div', class_='property-card')\n",
    "\n",
    "    for hostel_element in hostel_elements:\n",
    "        details = {}\n",
    "        \n",
    "        # Extract hostel details\n",
    "        details['Hostel Name'] = hostel_element.find('h2', class_='title-2').text.strip()\n",
    "        details['Distance from City Centre'] = hostel_element.find('span', class_='description').text.strip()\n",
    "        details['Ratings'] = hostel_element.find('div', class_='score orange').text.strip()\n",
    "        details['Total Reviews'] = hostel_element.find('div', class_='reviews').text.strip()\n",
    "        details['Overall Reviews'] = hostel_element.find('div', class_='keyword').text.strip()\n",
    "        details['Privates From Price'] = hostel_element.find('div', class_='price-col').text.strip()\n",
    "        details['Dorms From Price'] = hostel_element.find('div', class_='price-col').find_next('div', class_='price-col').text.strip()\n",
    "        \n",
    "        # Extract facilities\n",
    "        facilities_element = hostel_element.find('div', class_='facilities-container')\n",
    "        facilities = [item.text.strip() for item in facilities_element.find_all('div', class_='facilities')]\n",
    "        details['Facilities'] = ', '.join(facilities)\n",
    "\n",
    "        # Extract property description\n",
    "        details['Property Description'] = hostel_element.find('div', class_='rating-factors prop-card-tablet rating-factors small').text.strip()\n",
    "        \n",
    "        # Append hostel details to the list\n",
    "        hostels.append(details)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(hostels)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('london_hostels.csv', index=False)\n",
    "    print(\"Details of hostels in London have been saved to 'london_hostels.csv'.\")\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve data from hostelworld.com.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3799f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3866d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
